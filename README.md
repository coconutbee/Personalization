# Personalization Evaluation Benchmark

This project evaluates whether personalized portrait generation results match the text prompt and preserve identity. The pipeline covers expression classification, scenario consistency, gender, pose, and ID similarity, then outputs a weighted final score.

## Project Goals
1. Generate GT labels (expression, gender, pose) from prompts.
2. Use a VLM to classify expression and score scenario consistency.
3. Use MiVOLO to infer gender.
4. Use a pose model to predict head/body pose and compare to GT.
5. Use AdaFace to compute ID similarity (cosine similarity).
6. Aggregate scores with weights to produce the final report.

## Configuration and Structure
- prompts.json: input prompts and image list.
- faceswap_results/: generated/swapped images.
  - pixart/, janus/, infinity/, showo2/: per-method outputs.
  - reference/: reference face images for ID similarity.
- output/: fallback output path (used when method not matched).
- run.sh: one-click full evaluation.
- gt.json: GT labels generated from prompts.json.
- gt_scored.json: fully scored output.
- final_scores.csv: summary report.

> Image naming
> - Default output filename uses 0_{image}.png, e.g., image=1.jpg → 0_1.png.
> - Scripts try multiple extensions/prefixes, but keeping 0_{id}.png is recommended.

## Input Formats

### prompts.json
```json
[
  {"id": 0, "image": "0.jpg", "prompt": "A boy looks upward..."}
]
```
Fields:
- id: sample ID.
- image: image filename used to locate generated images.
- prompt: text prompt.

### gt.json (generated by gt_maker.py)
Added fields on top of the input:
- gt_expression: expression label (happy/surprise/confuse/neutral/sad/others)
- gt_gender: gender label (Male/Female/Both/Unknown)
- gt_pose: pose label (Frontal/Head_Turn_Left/…)

## Outputs

### gt.json (updated fields)
The pipeline incrementally writes:
- vlm_expression: VLM expression prediction
- expression_correct: expression correctness (0/1)
- scenario_reasoning: scenario consistency reasoning
- scenario_score: scenario score (0.0~1.0)
- mivolo_gender: MiVOLO gender prediction
- gender_correct: gender correctness (0/1)
- pose_prediction: pose prediction
- pose_correct: pose correctness (0/1)
- id_similarity: ID similarity (0.0~1.0)
- final_score: weighted final score (0~100)

### final_scores.csv
Aggregated per-image scores and overall averages.

## Weights
The final score is in $0\sim100$ with these weights:
- Expression: 0.17
- Scenario: 0.17
- Gender: 0.17
- Pose: 0.17
- ID Similarity: 0.32

See scoring.py for implementation.

## Inference

### 1) One-click
```bash
bash run.sh
```
run.sh executes:
1. gt_maker.py: generate gt.json from prompts.json
2. exps_scenario.py: InternVL expression + scenario
3. MiVOLO/gender_label.py: gender
4. AdaFace/inference.py: ID similarity
5. pose/eval_pose.py: pose evaluation
6. scoring.py: weighted scoring and reports

### 2) Method-specific
Scripts support a method parameter (pixart/janus/infinity/showo2) to switch image folders:
```bash
python exps_scenario.py --method pixart --json gt.json
python MiVOLO/gender_label.py --method pixart --json gt.json
python AdaFace/inference.py --method pixart --json gt.json
python pose/eval_pose.py --method pixart --json gt.json
```

### 3) Scoring only
```bash
python scoring.py --json gt.json --csv final_scores.csv
```

## Key Dependencies
- InternVL (lmdeploy pipeline): expression + scenario
- MiVOLO: gender
- AdaFace: ID similarity
- Pose model: pose classification

Install dependencies as needed. For package versions, see requirements.txt. Some scripts switch timm versions for model compatibility.
